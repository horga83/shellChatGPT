'\" t
.\" Automatically generated by Pandoc 3.1.2
.\"
.\" Define V font for inline verbatim, using C font in formats
.\" that render this, and otherwise B font.
.ie "\f[CB]x\f[]"x" \{\
. ftr V B
. ftr VI BI
. ftr VB B
. ftr VBI BI
.\}
.el \{\
. ftr V CR
. ftr VI CI
. ftr VB CB
. ftr VBI CBI
.\}
.TH "CHATGPT.SH" "1" "June 2023" "v0.14.5" "General Commands Manual"
.hy
.SS NAME
.PP
\ \ \ chatgpt.sh -- ChatGPT / DALL-E / Whisper Shell Wrapper
.SS SYNOPSIS
.PP
\ \ \ \f[B]chatgpt.sh\f[R] [\f[V]-c\f[R]|\f[V]-d\f[R]] [\f[V]opt\f[R]]
[\f[I]PROMPT|TXT_FILE\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-e\f[R] [\f[V]opt\f[R]]
[\f[I]INSTRUCTION\f[R]] [\f[I]INPUT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-i\f[R] [\f[V]opt\f[R]]
[\f[I]S\f[R]|\f[I]M\f[R]|\f[I]L\f[R]] [\f[I]PROMPT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-i\f[R] [\f[V]opt\f[R]]
[\f[I]S\f[R]|\f[I]M\f[R]|\f[I]L\f[R]] [\f[I]PNG_FILE\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-i\f[R] [\f[V]opt\f[R]]
[\f[I]S\f[R]|\f[I]M\f[R]|\f[I]L\f[R]] [\f[I]PNG_FILE\f[R]]
[\f[I]MASK_FILE\f[R]] [\f[I]PROPMT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-TTT\f[R] [-v]
[\f[V]-m\f[R][\f[I]MODEL\f[R]|\f[I]ENCODING\f[R]]]
[\f[I]TEXT\f[R]|\f[I]FILE\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-w\f[R] [\f[V]opt\f[R]]
[\f[I]AUDIO_FILE\f[R]] [\f[I]LANG\f[R]] [\f[I]PROMPT\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-W\f[R] [\f[V]opt\f[R]]
[\f[I]AUDIO_FILE\f[R]] [\f[I]PROMPT-EN\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-ccw\f[R] [\f[V]opt\f[R]]
[\f[I]LANG\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-ccW\f[R] [\f[V]opt\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-HH\f[R]
[\f[V]/\f[R]\f[I]SESSION_NAME\f[R]]
.PD 0
.P
.PD
\ \ \ \f[B]chatgpt.sh\f[R] \f[V]-ll\f[R] [\f[I]MODEL_NAME\f[R]]
.SS DESCRIPTION
.PP
Complete INPUT text when run without any options (single-turn, plain
text completions).
.PP
Positional arguments are read as a single \f[B]PROMPT\f[R].
Model \f[B]INSTRUCTION\f[R] is usually optional, however if it is
mandatory for a chosen model (such as edits models), then the first
positional argument is read as \f[B]INSTRUCTION\f[R] and the following
ones as \f[B]INPUT\f[R] or \f[B]PROMPT\f[R].
.PP
\f[V]Option -d\f[R] starts a multi-turn session in \f[B]plain text
completions\f[R].
This does not set further options automatically.
.PP
Set \f[V]option -c\f[R] to start a multi-turn chat mode via \f[B]text
completions\f[R] and record conversation.
This option accepts various models, defaults to
\f[I]text-davinci-003\f[R] if none set.
.PP
Set \f[V]option -cc\f[R] to start the chat mode via \f[B]native chat
completions\f[R] and use turbo models.
While in chat mode, some options are automatically set to un-lobotomise
the bot.
.PP
Set \f[V]option -C\f[R] to \f[B]resume\f[R] from last history session.
.PP
If the first positional argument of the script starts with the command
operator, the command \[lq]\f[V]/session\f[R] [\f[I]HIST_NAME\f[R]]\[rq]
to change to or create a new history file is assumed (with
\f[V]options -ccCdHH\f[R]).
.PP
Set model with \[lq]\f[V]-m\f[R] [\f[I]NAME\f[R]]\[rq] (full model
name).
Some models have an equivalent \f[I]INDEX\f[R] as short-hand, so
\[lq]\f[V]-m\f[R]\f[I]text-davinci-003\f[R]\[rq] and
\[lq]\f[V]-m\f[R]\f[I]0\f[R]\[rq] set the same model (list model by
\f[I]NAME\f[R] with \f[V]option -l\f[R] or by \f[I]INDEX\f[R] with
\f[V]option -ll\f[R]).
.PP
Set \f[I]maximum response tokens\f[R] with \f[V]option\f[R]
\[lq]\f[V]-\f[R]NUM\[rq] or \[lq]\f[V]-M\f[R] NUM\[rq].
This defaults to 512 tokens in chat and single-turn modes.
.PP
\f[I]Model capacity\f[R] (max model tokens) can be set with a second
\f[I]NUM\f[R] such as \[lq]\f[V]-\f[R]\f[I]NUM,NUM\f[R]\[rq] or
\[lq]\f[V]-M\f[R] NUM-NUM\[rq], otherwise it is set automatically to the
capacity of known models, or to \f[I]2048\f[R] tokens as fallback.
.PP
If a plain text file path is set as first positional argument, it is
loaded as text PROMPT (text cmpls, chat cmpls, and text/code edits).
.PP
\f[V]Option -S\f[R] sets an INSTRUCTION prompt (the initial prompt) for
text cmpls, chat cmpls, and text/code edits.
A text file path may be supplied as the single argument.
.PP
If the argument to \f[V]-S\f[R] option starts with a backslash or a
percent sign, such as \[lq]\f[V]-S\f[R]
\f[V]/\f[R]\f[I]linux_terminal\f[R]\[rq], start search for an
\f[I]awesome-chatgpt-prompt(-zh)\f[R] (by Fatih KA and PlexPt).
Set \[lq]\f[V]//\f[R]\[rq] or \[lq]\f[V]%%\f[R]\[rq] to refresh cache.
Use with \f[I]davinci\f[R] and \f[I]gpt-3.5+\f[R] models.
.PP
\f[V]Option -e\f[R] sets the \f[B]text edits\f[R] endpoint.
That endpoint requires both INSTRUCTION and INPUT prompts.
User may choose a model amongst the \f[I]edit model family\f[R].
.PP
\f[V]Option -i\f[R] \f[B]generates images\f[R] according to text PROMPT.
If the first positional argument is an \f[I]IMAGE\f[R] file, then
\f[B]generate variations\f[R] of it.
If the first positional argument is an \f[I]IMAGE\f[R] file and the
second a \f[I]MASK\f[R] file (with alpha channel and transparency), and
a text PROMPT (required), then \f[B]edit the\f[R] \f[I]IMAGE\f[R]
according to \f[I]MASK\f[R] and PROMPT.
If \f[I]MASK\f[R] is not provided, \f[I]IMAGE\f[R] must have
transparency.
.PP
Optionally, size of output image may be set with
\[lq][\f[I]S\f[R]]\f[I]mall\f[R]\[rq],
\[lq][\f[I]M\f[R]]\f[I]edium\f[R]\[rq] or
\[lq][\f[I]L\f[R]]\f[I]arge\f[R]\[rq] as the first positional argument.
See \f[B]IMAGES section\f[R] below for more information on
\f[B]inpaint\f[R] and \f[B]outpaint\f[R].
.PP
\f[V]Option -w\f[R] \f[B]transcribes audio\f[R] from \f[I]mp3\f[R],
\f[I]mp4\f[R], \f[I]mpeg\f[R], \f[I]mpga\f[R], \f[I]m4a\f[R],
\f[I]wav\f[R], and \f[I]webm\f[R] files.
First positional argument must be an \f[I]AUDIO\f[R] file.
Optionally, set a \f[I]TWO-LETTER\f[R] input language
(\f[I]ISO-639-1\f[R]) as second argument.
A PROMPT may also be set to guide the model\[cq]s style or continue a
previous audio segment.
The prompt should match the audio language.
.PP
\f[V]Option -W\f[R] \f[B]translates audio\f[R] stream to \f[B]English
text\f[R].
A PROMPT in English may be set to guide the model as the second
positional argument.
.PP
Combine \f[V]-wW\f[R] \f[B]with\f[R] \f[V]-cc\f[R] to start \f[B]chat
with voice input\f[R] (Whisper) support.
Output may be piped to a voice synthesiser to have a full voice in and
out experience.
.PP
\f[V]Option -y\f[R] sets python tiktoken instead of the default script
hack to preview token count.
This option makes token count preview accurate however it is slow.
Useful for rebuilding history context independently from the original
model used to generate responses.
.PP
Stdin is supported when there is no positional arguments left after
option parsing.
Stdin input sets a single PROMPT.
.PP
User configuration is kept at \[lq]\f[I]\[ti]/.chatgpt.conf\f[R]\[rq].
Script cache is kept at \[lq]\f[I]\[ti]/.cache/chatgptsh\f[R]\[rq].
.PP
A personal (free) OpenAI API is required, set it with \f[V]-K\f[R].
Also, see \f[B]ENVIRONMENT section\f[R].
.PP
See the online man page and script usage examples at:
<https://github.com/mountaineerbr/shellChatGPT/tree/main>.
.PP
For complete model and settings information, refer to OpenAI API docs at
<https://platform.openai.com/docs/>.
.SS TEXT / CHAT COMPLETIONS
.SS 1. Text completions
.PP
Given a prompt, the model will return one or more predicted completions.
For example, given a partial input, the language model will try
completing it until probable \[lq]\f[V]<|endoftext|>\f[R]\[rq], or other
stop sequences (stops may be set with \f[V]-s\f[R]).
.PP
\f[B]Restart\f[R] and \f[B]start sequences\f[R] may be optionally set
and are always preceded by a new line.
.PP
To enable \f[B]multiline input\f[R], type in a backslash
\[lq]\f[I]\[rs]\f[R]\[rq] as the last character of the input line and
press ENTER (backslash will be removed from input), or set
\f[V]option -u\f[R].
Once enabled, press ENTER twice to confirm the multiline prompt.
Useful to paste from clipboard, but empty lines will confirm the prompt
up to that point.
.PP
Language model \f[B]SKILLS\f[R] can activated, with specific prompts,
see <https://platform.openai.com/examples>.
.SS 2. Chat Mode
.SS 2.1 Text Completions Chat
.PP
Set \f[V]option -c\f[R] to start chat mode of text completions.
It keeps a history file, and keeps new questions in context.
This works with a variety of models.
.SS 2.2 Native Chat Completions
.PP
Set the double \f[V]option -cc\f[R] to start chat completions mode.
Turbo models are also the best option for many non-chat use cases.
.SS 2.3 Q & A Format
.PP
The defaults chat format is \[lq]\f[B]Q & A\f[R]\[rq].
The \f[B]restart sequence\f[R] \[lq]\[rs]n_Q:\ \f[I]\[rq] and the
\f[BI]start text\f[I] \[rq]\[rs]n_A:\f[R]\[rq] are injected for the chat
bot to work well with text cmpls.
.PP
In native chat completions, setting a prompt with \[lq]\f[I]:\f[R]\[rq]
as the initial character sets the prompt as a \f[B]SYSTEM\f[R] message.
In text completions, however, typing a colon \[lq]\f[I]:\f[R]\[rq] at
the start of the prompt causes the text following it to be appended
immediately to the last (response) prompt text.
.SS 2.4 Chat Commands
.PP
While in chat mode, the following commands can be typed in the new
prompt to set a new parameter.
The command operator may be either \[lq]\f[V]!\f[R]\[rq], or
\[lq]\f[V]/\f[R]\[rq].
.PP
.TS
tab(@);
l l l.
T{
Model
T}@T{
Settings
T}@T{
T}
_
T{
\f[V]!NUM\f[R]
T}@T{
\f[V]!max\f[R] [\f[I]NUM\f[R],\f[I]NUM\f[R]]
T}@T{
Set response tokens / model capacity.
T}
T{
\f[V]-a\f[R]
T}@T{
\f[V]!pre\f[R] [\f[I]VAL\f[R]]
T}@T{
Set presence pensalty.
T}
T{
\f[V]-A\f[R]
T}@T{
\f[V]!freq\f[R] [\f[I]VAL\f[R]]
T}@T{
Set frequency penalty.
T}
T{
\f[V]-m\f[R]
T}@T{
\f[V]!mod\f[R] [\f[I]MOD\f[R]|\f[I]IND\f[R]]
T}@T{
Set model (by index or name).
T}
T{
\f[V]-p\f[R]
T}@T{
\f[V]!top\f[R] [\f[I]VAL\f[R]]
T}@T{
Set top_p.
T}
T{
\f[V]-r\f[R]
T}@T{
\f[V]!restart\f[R] [\f[I]SEQ\f[R]]
T}@T{
Set restart sequence.
T}
T{
\f[V]-R\f[R]
T}@T{
\f[V]!start\f[R] [\f[I]SEQ\f[R]]
T}@T{
Set start sequence.
T}
T{
\f[V]-s\f[R]
T}@T{
\f[V]!stop\f[R] [\f[I]SEQ\f[R]]
T}@T{
Set one stop sequence.
T}
T{
\f[V]-t\f[R]
T}@T{
\f[V]!temp\f[R] [\f[I]VAL\f[R]]
T}@T{
Set temperature.
T}
T{
\f[V]-w\f[R]
T}@T{
\f[V]!rec\f[R]
T}@T{
Start audio record chat.
T}
.TE
.PP
.TS
tab(@);
l l l.
T{
Script
T}@T{
Settings
T}@T{
T}
_
T{
\f[V]-o\f[R]
T}@T{
\f[V]!clip\f[R]
T}@T{
Copy responses to clipboard.
T}
T{
\f[V]-u\f[R]
T}@T{
\f[V]!multi\f[R]
T}@T{
Toggle multiline prompter.
T}
T{
\f[V]-v\f[R]
T}@T{
\f[V]!ver\f[R]
T}@T{
Toggle verbose.
T}
T{
\f[V]-x\f[R]
T}@T{
\f[V]!ed\f[R]
T}@T{
Toggle text editor interface.
T}
T{
\f[V]-y\f[R]
T}@T{
\f[V]!tik\f[R]
T}@T{
Toggle python tiktoken use.
T}
T{
\f[V]!r\f[R]
T}@T{
\f[V]!regen\f[R]
T}@T{
Renegerate last response.
T}
T{
\f[V]!q\f[R]
T}@T{
\f[V]!quit\f[R]
T}@T{
Exit.
T}
.TE
.PP
.TS
tab(@);
l l l.
T{
Session
T}@T{
Management
T}@T{
T}
_
T{
\f[V]-c\f[R]
T}@T{
\f[V]!new\f[R]
T}@T{
Start new session.
T}
T{
\f[V]-H\f[R]
T}@T{
\f[V]!hist\f[R]
T}@T{
Edit history in editor.
T}
T{
\f[V]-L\f[R]
T}@T{
\f[V]!log\f[R] [\f[I]FILEPATH\f[R]]
T}@T{
Save to log file.
T}
T{
\f[V]!c\f[R]
T}@T{
\f[V]!copy\f[R] [\f[I]SRC_HIST\f[R]] [\f[I]DEST_HIST\f[R]]
T}@T{
Copy session from source to destination.
T}
T{
\f[V]!f\f[R]
T}@T{
\f[V]!fork\f[R] [\f[I]DEST_HIST\f[R]]
T}@T{
Fork current session to destination.
T}
T{
\f[V]!s\f[R]
T}@T{
\f[V]!session\f[R] [\f[I]HIST_FILE\f[R]]
T}@T{
Change to, search or create hist file.
T}
T{
\f[V]!!s\f[R]
T}@T{
\f[V]!!session\f[R] [\f[I]HIST_FILE\f[R]]
T}@T{
Same as \f[V]!session\f[R], break session.
T}
T{
T}@T{
\f[V]!sub\f[R]
T}@T{
Copy session to tail.
T}
T{
T}@T{
\f[V]!list\f[R]
T}@T{
List history files.
T}
.TE
.PP
E.g.: \[lq]\f[V]/temp\f[R] \f[I]0.7\f[R]\[rq],
\[lq]\f[V]!mod\f[R]\f[I]1\f[R]\[rq], \[lq]\f[V]-p\f[R]
\f[I]0.2\f[R]\[rq], and \[lq]\f[V]/s\f[R] \f[I]hist_name\f[R]\[rq].
.SS Session Management
.PP
The script uses a \f[I]TSV file\f[R] to record entries, which is kept at
the script cache directory.
A new history file can be created, or an existing one changed to with
command \[lq]\f[V]/session\f[R] [\f[I]HIST_FILE\f[R]]\[rq], in which
\f[I]HIST_FILE\f[R] is the file name of, or path to a tsv file with or
without the \f[I].tsv\f[R] extension.
.PP
A history file can contain many sessions.
The last one (the tail session) is always read if the resume
\f[V]option -C\f[R] is set.
To continue a previous session than the tail session of history file,
run chat command \[lq]\f[V]/copy\f[R] [\f[I]SRC_HIST_FILE\f[R]]
[\f[I]DEST_HIST_FILE\f[R]]\[rq].
.PP
It is also possible to copy a session of a history file to another one.
.PP
If \[lq]\f[V]/copy\f[R] \f[I]current\f[R]\[rq] is run, select a session
to copy to the tail of the current history file and resume.
.PP
In order to change the chat context at run time, the history file may be
edited with the \[lq]\f[V]!hist\f[R]\[rq] command.
Delete history entries or comment them out with \[lq]\f[V]#\f[R]\[rq].
.SS 2.5 Completion Preview / Regeneration
.PP
To preview a prompt completion before commiting it to history, append a
forward slash \[lq]\f[V]/\f[R]\[rq] to the prompt as the last character.
Regenerate it again or press ENTER to accept it.
.PP
After a response has been written to the history file,
\f[B]regenerate\f[R] it with command \[lq]\f[V]!regen\f[R]\[rq] or type
in a single forward slash in the new empty prompt.
.SS 3. Prompt Engineering and Design
.PP
Minimal \f[B]INSTRUCTION\f[R] to behave like a chatbot is given with
chat \f[V]options -cc\f[R], unless otherwise explicitly set by the user.
.PP
On chat mode, if no INSTRUCTION is set, minimal instruction is given,
and some options auto set, such as increasing temp and presence penalty,
in order to un-lobotomise the bot.
With cheap and fast models of text cmpls, such as Curie, the best_of
option may be worth setting (to 2 or 3).
.PP
Prompt engineering is an art on itself.
Study carefully how to craft the best prompts to get the most out of
text, code and chat compls models.
.PP
Certain prompts may return empty responses.
Maybe the model has nothing to further complete input or it expects more
text.
Try trimming spaces, appending a full stop/ellipsis, resetting
temperature, or adding more text.
.PP
Prompts ending with a space character may result in lower quality
output.
This is because the API already incorporates trailing spaces in its
dictionary of tokens.
.PP
Note that the model\[cq]s steering and capabilities require prompt
engineering to even know that it should answer the questions.
.PP
It is also worth trying to sample 3 - 5 times (setting
\f[V]best_of\f[R], for example) in order to obtain a good response.
.PP
For more on prompt design, see:
.IP \[bu] 2
<https://platform.openai.com/docs/guides/completion/prompt-design>
.IP \[bu] 2
<https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md>
.PP
See detailed info on settings for each endpoint at:
.IP \[bu] 2
<https://platform.openai.com/docs/>
.SS CODE COMPLETIONS
.PP
Codex models are discontinued.
Use davinci or turbo models for coding tasks.
.PP
Turn comments into code, complete the next line or function in context,
add code comments, and rewrite code for efficiency, amongst other
functions.
.PP
Start with a comment with instructions, data or code.
To create useful completions it\[cq]s helpful to think about what
information a programmer would need to perform a task.
.SS TEXT EDITS
.PP
This endpoint is set with models with \f[B]edit\f[R] in their name or
\f[V]option -e\f[R].
Editing works by setting INSTRUCTION on how to modify a prompt and the
prompt proper.
.PP
The edits endpoint can be used to change the tone or structure of text,
or make targeted changes like fixing spelling.
Edits work well on empty prompts, thus enabling text generation similar
to the completions endpoint.
.SS IMAGES / DALL-E
.SS 1. Image Generations
.PP
An image can be created given a text prompt.
A text PROMPT of the desired image(s) is required.
The maximum length is 1000 characters.
.SS 2. Image Variations
.PP
Variations of a given \f[I]IMAGE\f[R] can be generated.
The \f[I]IMAGE\f[R] to use as the basis for the variations must be a
valid PNG file, less than 4MB and square.
.SS 3. Image Edits
.PP
To edit an \f[I]IMAGE\f[R], a \f[I]MASK\f[R] file may be optionally
provided.
If \f[I]MASK\f[R] is not provided, \f[I]IMAGE\f[R] must have
transparency, which will be used as the mask.
A text prompt is required.
.SS 3.1 ImageMagick
.PP
If \f[B]ImageMagick\f[R] is available, input \f[I]IMAGE\f[R] and
\f[I]MASK\f[R] will be checked and processed to fit dimensions and other
requirements.
.SS 3.2 Transparent Colour and Fuzz
.PP
A transparent colour must be set with
\[lq]\f[V]-\[at]\f[R][\f[I]COLOUR\f[R]]\[rq] to create the mask.
Defaults=\f[I]black\f[R].
.PP
By defaults, the \f[I]COLOUR\f[R] must be exact.
Use the \f[V]fuzz option\f[R] to match colours that are close to the
target colour.
This can be set with \[lq]\f[V]-\[at]\f[R][\f[I]VALUE%\f[R]]\[rq] as a
percentage of the maximum possible intensity, for example
\[lq]\f[V]-\[at]\f[R]\f[I]10%black\f[R]\[rq].
.PP
See also:
.IP \[bu] 2
<https://imagemagick.org/script/color.php>
.IP \[bu] 2
<https://imagemagick.org/script/command-line-options.php#fuzz>
.SS 3.3 Mask File / Alpha Channel
.PP
An alpha channel is generated with \f[B]ImageMagick\f[R] from any image
with the set transparent colour (defaults to \f[I]black\f[R]).
In this way, it is easy to make a mask with any black and white image as
a template.
.SS 3.4 In-Paint and Out-Paint
.PP
In-painting is achieved setting an image with a MASK and a prompt.
.PP
Out-painting can also be achieved manually with the aid of this script.
Paint a portion of the outer area of an image with \f[I]alpha\f[R], or a
defined \f[I]transparent\f[R] \f[I]colour\f[R] which will be used as the
mask, and set the same \f[I]colour\f[R] in the script with
\f[V]-\[at]\f[R].
Choose the best result amongst many results to continue the out-painting
process step-wise.
.PP
Optionally, for all image generations, variations, and edits, set
\f[B]size of output image\f[R] with \[lq]\f[I]256x256\f[R]\[rq]
(\[lq]\f[I]Small\f[R]\[rq]), \[lq]\f[I]512x512\f[R]\[rq]
(\[lq]\f[I]Medium\f[R]\[rq]), or \[lq]\f[I]1024x1024\f[R]\[rq]
(\[lq]\f[I]Large\f[R]\[rq]) as the first positional argument.
Defaults=\f[I]512x512\f[R].
.SS AUDIO / WHISPER
.SS 1. Transcriptions
.PP
Transcribes audio file or voice record into the input language.
Set a \f[I]two-letter\f[R] \f[I]ISO-639-1\f[R] language code
(\f[I]en\f[R], \f[I]es\f[R], \f[I]ja\f[R], or \f[I]zh\f[R]) as the
positional argument following the input audio file.
A prompt may also be set as last positional parameter to help guide the
model.
This prompt should match the audio language.
.SS 2. Translations
.PP
Translates audio into \f[B]English\f[R].
An optional text to guide the model\[cq]s style or continue a previous
audio segment is optional as last positional argument.
This prompt should be in English.
.PP
Setting \f[B]temperature\f[R] has an effect, the higher the more random.
.SS QUOTING AND SPECIAL SYMBOLS
.PP
The special sequences (\f[V]\[rs]b\f[R], \f[V]\[rs]f\f[R],
\f[V]\[rs]n\f[R], \f[V]\[rs]r\f[R], \f[V]\[rs]t\f[R] and
\f[V]\[rs]uHEX\f[R]) are interpreted as quoted \f[I]backspace\f[R],
\f[I]form feed\f[R], \f[I]new line\f[R], \f[I]return\f[R], \f[I]tab\f[R]
and \f[I]unicode hex\f[R].
To preserve these symbols as literals instead (e.
g.
\f[B]Latex syntax\f[R]), type in an extra backslash such as
\[lq]\f[V]\[rs]\[rs]theta\f[R]\[rq].
.SS ENVIRONMENT
.PP
\f[B]CHATGPTRC\f[R]
.TP
\f[B]CONFFILE\f[R]
Path to user \f[I]chatgpt.sh configuration\f[R].
.RS
.PP
Defaults=\[dq]\f[I]\[ti]/.chatgpt.conf\f[R]\[dq]
.RE
.TP
\f[B]FILECHAT\f[R]
Path to a script-formatted TSV history file to read from.
.TP
\f[B]INSTRUCTION\f[R]
Initial instruction set for the chatbot.
.PP
\f[B]OPENAI_API_KEY\f[R]
.TP
\f[B]OPENAI_KEY\f[R]
Set your personal (free) OpenAI API key.
.TP
\f[B]REC_CMD\f[R]
Audio recording command.
.PP
\f[B]VISUAL\f[R]
.TP
\f[B]EDITOR\f[R]
Text editor for external prompt editing.
.RS
.PP
Defaults=\[dq]\f[I]vim\f[R]\[dq]
.RE
.SS BUGS
.PP
Changing models in the same session may generate token count errors
because the token count recorded in history file entries may differ
significantly from model to model (encoding).
.PP
With the exception of Davinci models, older models were designed to be
run as one-shot.
.PP
Instruction prompts are required for the model to even know that it
should answer questions.
.PP
Garbage in, garbage out.
An idiot savant.
.SS REQUIREMENTS
.PP
A free OpenAI \f[B]API key\f[R].
\f[V]Bash\f[R], \f[V]cURL\f[R], and \f[V]JQ\f[R].
.PP
\f[V]ImageMagick\f[R], and
\f[V]Sox\f[R]/\f[V]Alsa-tools\f[R]/\f[V]FFmpeg\f[R] are optionally
required.
.SS OPTIONS
.SS Model Settings
.TP
\f[B]-\[at]\f[R] [[\f[I]VAL%\f[R]]\f[I]COLOUR\f[R]], \f[B]--alpha\f[R]=[[\f[I]VAL%\f[R]]\f[I]COLOUR\f[R]]
Set transparent colour of image mask.
Def=\f[I]black\f[R].
.RS
.PP
Fuzz intensity can be set with [VAL%].
Def=\f[I]0%\f[R].
.RE
.PP
\f[B]-NUM\f[R]
.TP
\f[B]-M\f[R] [\f[I]NUM\f[R][\f[I]-NUM\f[R]]], \f[B]--max-tokens\f[R]=[\f[I]NUM\f[R][\f[I]-NUM\f[R]]]
Set maximum number of \f[I]response tokens\f[R].
Def=\f[I]512\f[R].
.RS
.PP
\f[I]Model capacity\f[R] can be set with a second number.
Def=\f[I]auto-512\f[R].
.RE
.TP
\f[B]-a\f[R] [\f[I]VAL\f[R]], \f[B]--presence-penalty\f[R]=[\f[I]VAL\f[R]]
Set presence penalty (cmpls/chat, -2.0 - 2.0).
.TP
\f[B]-A\f[R] [\f[I]VAL\f[R]], \f[B]--frequency-penalty\f[R]=[\f[I]VAL\f[R]]
Set frequency penalty (cmpls/chat, -2.0 - 2.0).
.TP
\f[B]-b\f[R] [\f[I]VAL\f[R]], \f[B]--best-of\f[R]=[\f[I]VAL\f[R]]
Set best of, must be greater than \f[V]option -n\f[R] (cmpls).
Def=\f[I]1\f[R].
.TP
\f[B]-B\f[R], \f[B]--log-prob\f[R]
Print log probabilities to stderr (cmpls, 0 - 5).
.TP
\f[B]-m\f[R] [\f[I]MOD\f[R]], \f[B]--model\f[R]=[\f[I]MOD\f[R]]
Set model by \f[I]NAME\f[R].
.TP
\f[B]-m\f[R] [\f[I]IND\f[R]]
Set model by \f[I]INDEX\f[R]:
.PP
.TS
tab(@);
l l l.
T{
T}@T{
\f[B]COMPLETIONS\f[R]
T}@T{
\f[B]EDITS\f[R]
T}
T{
T}@T{
\f[I]0\f[R].
text-davinci-003
T}@T{
\f[I]8\f[R].
text-davinci-edit-001
T}
T{
T}@T{
\f[I]1\f[R].
text-curie-001
T}@T{
\f[I]9\f[R].
code-davinci-edit-001
T}
T{
T}@T{
\f[I]2\f[R].
text-babbage-001
T}@T{
\f[B]CHAT\f[R]
T}
T{
T}@T{
\f[I]3\f[R].
text-ada-001
T}@T{
\f[I]10\f[R].
gpt-3.5-turbo
T}
T{
T}@T{
\f[I]4\f[R].
davinci
T}@T{
\f[B]AUDIO\f[R]
T}
T{
T}@T{
\f[I]5\f[R].
curie
T}@T{
\f[I]11\f[R].
whisper-1
T}
T{
T}@T{
\f[B]MODERATION\f[R]
T}@T{
\f[B]GPT-4\f[R]
T}
T{
T}@T{
\f[I]6\f[R].
text-moderation-latest
T}@T{
\f[I]12\f[R].
gpt-4
T}
T{
T}@T{
\f[I]7\f[R].
text-moderation-stable
T}@T{
\f[I]13\f[R].
gpt-4-32k
T}
.TE
.TP
\f[B]-n\f[R] [\f[I]NUM\f[R]], \f[B]--results\f[R]=[\f[I]NUM\f[R]]
Set number of results.
Def=\f[I]1\f[R].
.TP
\f[B]-p\f[R] [\f[I]VAL\f[R]], \f[B]--top-p\f[R]=[\f[I]VAL\f[R]]
Set Top_p value, nucleus sampling (cmpls/chat, 0.0 - 1.0).
.TP
\f[B]-r\f[R] [\f[I]SEQ\f[R]], \f[B]--restart-sequence\f[R]=[\f[I]SEQ\f[R]]
Set restart sequence string (cmpls).
.TP
\f[B]-R\f[R] [\f[I]SEQ\f[R]], \f[B]--start-sequence\f[R]=[\f[I]SEQ\f[R]]
Set start sequence string (cmpls).
.TP
\f[B]-s\f[R] [\f[I]SEQ\f[R]], \f[B]--stop\f[R]=[\f[I]SEQ\f[R]]
Set stop sequences, up to 4.
Def=\[dq]\f[I]<|endoftext|>\f[R]\[dq].
.TP
\f[B]-S\f[R] [\f[I]INSTRUCTION\f[R]|\f[I]FILE\f[R]], \f[B]--instruction\f[R]
Set an instruction prompt.
It may be a text file.
.TP
\f[B]-t\f[R] [\f[I]VAL\f[R]], \f[B]--temperature\f[R]=[\f[I]VAL\f[R]]
Set temperature value (cmpls/chat/edits/audio), (0.0 - 2.0, whisper 0.0
- 1.0).
Def=\f[I]0\f[R].
.SS Script Modes
.TP
\f[B]-c\f[R], \f[B]--chat\f[R]
Chat mode in text completions, session break.
.TP
\f[B]-cc\f[R]
Chat mode in chat completions, session break.
.TP
\f[B]-C\f[R], \f[B]--continue\f[R], \f[B]--resume\f[R]
Continue (resume) from last session (compls/chat).
.TP
\f[B]-d\f[R], \f[B]--text\f[R]
Start new multi-turn session in plain text completions.
.TP
\f[B]-e\f[R] [\f[I]INSTRUCTION\f[R]] [\f[I]INPUT\f[R]], \f[B]--edit\f[R]
Set Edit mode.
Model def=\f[I]text-davinci-edit-001\f[R].
.TP
\f[B]-i\f[R] [\f[I]PROMPT\f[R]], \f[B]--image\f[R]
Generate images given a prompt.
.TP
\f[B]-i\f[R] [\f[I]PNG\f[R]]
Create variations of a given image.
.TP
\f[B]-i\f[R] [\f[I]PNG\f[R]] [\f[I]MASK\f[R]] [\f[I]PROMPT\f[R]]
Edit image with mask and prompt (required).
.TP
\f[B]-q\f[R], \f[B]--insert\f[R]
Insert text rather than completing only.
.RS
.PP
Use \[lq]\f[I][insert]\f[R]\[rq] to indicate where the language model
should insert text (cmpls).
.RE
.PP
\f[B]-S\f[R] \f[V]/\f[R][\f[I]AWESOME_PROMPT_NAME\f[R]]
.TP
\f[B]-S\f[R] \f[V]%\f[R][_AWESOME_PROMPT_NAME_ZH]
Set or search an \f[I]awesome-chatgpt-prompt(-zh)\f[R]
(\f[I]davinci\f[R] and \f[I]gpt3.5+\f[R] models).
.RS
.PP
Set \f[V]//\f[R] or \f[V]%%\f[R] instead to refresh cache.
.RE
.TP
\f[B]-TTT\f[R], \f[B]--tiktoken\f[R]
Count input tokens with python tiktoken (ignores special tokens).
It heeds \f[V]options -ccm\f[R].
.RS
.PP
Set twice to print tokens, thrice to available encodings.
.PP
Set model or encoding with \f[V]option -m\f[R].
.RE
.TP
\f[B]-w\f[R] [\f[I]AUD\f[R]] [\f[I]LANG\f[R]] [\f[I]PROMPT\f[R]], \f[B]--transcribe\f[R]
Transcribe audio file into text.
LANG is optional.
A prompt that matches the audio language is optional.
.RS
.PP
Set twice to get phrase-level timestamps.
.RE
.TP
\f[B]-W\f[R] [\f[I]AUD\f[R]] [\f[I]PROMPT-EN\f[R]], \f[B]--translate\f[R]
Translate audio file into English text.
.RS
.PP
Set twice to get phrase-level timestamps.
.RE
.SS Script Settings
.TP
\f[B]-f\f[R], \f[B]--no-config\f[R]
Ignore user config file and environment.
.TP
\f[B]-F\f[R]
Edit configuration file with text editor, if it exists.
.TP
\f[B]-h\f[R], \f[B]--help\f[R]
Print the help page.
.TP
\f[B]-H\f[R] [\f[V]/\f[R]\f[I]HIST_FILE\f[R]], \f[B]--hist\f[R]
Edit history file with text editor or pipe to stdout.
.RS
.PP
A history file name can be optionally set as argument.
.RE
.TP
\f[B]-HH\f[R] [\f[V]/\f[R]\f[I]HIST_FILE\f[R]]
Pretty print last history session to stdout.
.RS
.PP
Heeds \f[V]options -ccdrR\f[R] to print with the specified restart and
start sequences.
.RE
.TP
\f[B]-j\f[R], \f[B]--raw\f[R]
Print raw JSON response (debug with \f[V]-jVVz\f[R]).
.TP
\f[B]-k\f[R], \f[B]--no-colour\f[R]
Disable colour output.
Def=\f[I]auto\f[R].
.TP
\f[B]-K\f[R] [\f[I]KEY\f[R]], \f[B]--api-key\f[R]=[\f[I]KEY\f[R]]
Set OpenAI API key.
.TP
\f[B]-l\f[R] [\f[I]MOD\f[R]], \f[B]--list-models\f[R]
List models or print details of \f[I]MODEL\f[R].
.RS
.PP
Set twice to print script model indexes instead.
.RE
.TP
\f[B]-L\f[R] [\f[I]FILEPATH\f[R]], \f[B]--log\f[R]=[\f[I]FILEPATH\f[R]]
Set log file.
\f[I]FILEPATH\f[R] is required.
.TP
\f[B]-o\f[R], \f[B]--clipboard\f[R]
Copy response to clipboard.
.TP
\f[B]-u\f[R], \f[B]--multiline\f[R]
Toggle multiline prompter.
.TP
\f[B]-v\f[R], \f[B]--verbose\f[R]
Less verbose.
May set multiple times.
.TP
\f[B]-V\f[R]
Pretty-print context.
.RS
.PP
Set twice to dump raw request.
.RE
.TP
\f[B]-x\f[R], \f[B]--editor\f[R]
Edit prompt in text editor.
.TP
\f[B]-y\f[R], \f[B]--tik\f[R]
Set tiktoken for token preview (cmpls, chat).
.TP
\f[B]-z\f[R], \f[B]--last\f[R]
Print last response JSON data.
.TP
\f[B]-Z\f[R]
Run with Z-shell.
.SH AUTHORS
mountaineerbr.
